{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "\n",
    "`torvalds/linux` 같은 리파짓토리와 관련된 활동을 전개한 유저들만을 우선 뽑고, 그들의 활동 내역을 Knowledge Graph 형식으로 구성한 것입니다. Graph Embedding이 유효하게 동작하는지를 해당 Knowledge Graph 데이터로 확인해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_github_kg_dataset(name='linux'):\n",
    "    \"\"\"knowledge graph Dataset을 불러오는 함수\n",
    "    현재 3가지 github knowledge graph가 구성되어 있음\n",
    "    params\n",
    "    * name : choose [linux, tensorflow, vim]\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.utils import get_file\n",
    "    fpath = get_file(\n",
    "        \"github-playground.h5\",\n",
    "        \"https://storage.googleapis.com/github-playground/playground.h5\")\n",
    "    target_df = pd.read_hdf(fpath, key=name)    \n",
    "    \n",
    "    # Type을 String으로 합치기\n",
    "    type_df = pd.read_hdf(fpath, key='type')\n",
    "    target_df.type = target_df.type.map(type_df.type.to_dict())\n",
    "    \n",
    "    # Repository Name과 Repository ID를 합치기\n",
    "    repository_df = pd.read_hdf(fpath, key='repository')\n",
    "    df = pd.merge(target_df, repository_df)\n",
    "    return df\n",
    "\n",
    "\n",
    "# tensorflow, vim 도 가능합니다.\n",
    "df = load_github_kg_dataset(name='linux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***caution*** : 아래와 같은 에러가 발생시, 링크를 타고 수정해주세요\n",
    "\n",
    "* [ValueError: cannot set WRITEABLE flag to True of this array](https://github.com/pandas-dev/pandas/issues/24839)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th : 12973855\n",
      "2th : 12093033\n",
      "3th : 12092756\n",
      "4th : 12092685\n",
      "5th : 12092682\n"
     ]
    }
   ],
   "source": [
    "# column 이름 변경\n",
    "df.rename({\n",
    "    \"actor_id\": 'subject',\n",
    "    \"type\": 'relation', \n",
    "    \"repo_name\":\"object\"},axis=1,inplace=True)\n",
    "\n",
    "# Embedding에 활용할 relation type을 지정\n",
    "train_df = df[df.relation.isin([\n",
    "    'WatchEvent', 'IssuesEvent', 'PushEvent'])]\n",
    "\n",
    "# K-core Sampling 수행\n",
    "k_core = 5\n",
    "for i in range(1, 10):\n",
    "    prev_counts = len(train_df)    \n",
    "    print(f\"{i}th : {prev_counts}\")\n",
    "    \n",
    "    sub_counts = train_df.subject.value_counts()\n",
    "    obj_counts = train_df.object.value_counts()\n",
    "    train_df = train_df[\n",
    "        train_df.subject.isin(sub_counts[sub_counts>=k_core].index)\n",
    "        & train_df.object.isin(obj_counts[obj_counts>=k_core].index)]\n",
    "    \n",
    "    if prev_counts == len(train_df):\n",
    "        # 변화가 없으면 종료\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from Values to indices\n",
    "\n",
    "주어진 Node와 Edge들의 값을 Embedding Index값으로 변경합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set(train_df.subject.unique()) | set(train_df.object.unique())\n",
    "\n",
    "id2node = {i:node for i, node in enumerate(nodes)}\n",
    "node2id = {node:i for i, node in enumerate(nodes)}\n",
    "\n",
    "edges = set(train_df.relation.unique())\n",
    "\n",
    "id2edge = {i:edge for i, edge in enumerate(edges)}\n",
    "edge2id = {edge:i for i, edge in enumerate(edges)}\n",
    "\n",
    "# node와 edge를 모두 index로 변경\n",
    "subjects = train_df.subject.map(node2id).values\n",
    "relations = train_df.relation.map(edge2id).values\n",
    "objects = train_df.object.map(node2id).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupting function\n",
    "\n",
    "Graph Embedding에서는 Negative Triplet Sampling이 핵심입니다.<br>\n",
    "TransE에서는 존재하는 Triplet과 triplet의 head 혹은 tail을 무작위로 섞은 Negative Sampling을 동시에 학습시키게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_triplet(triplet):\n",
    "    \"\"\" 50% 확률로 head 혹은 tail을 corrupt한 negative sample을 추가\n",
    "    \"\"\"\n",
    "    mask = tf.random.uniform(tf.shape(triplet['pos_subject']))<0.5\n",
    "    \n",
    "    triplet['neg_subject'] = tf.where(\n",
    "        mask, triplet['pos_subject'], tf.random.shuffle(triplet['pos_subject']))\n",
    "    triplet['neg_object'] = tf.where(\n",
    "        mask, tf.random.shuffle(triplet['pos_object']), triplet['pos_object'])    \n",
    "    return triplet\n",
    "\n",
    "def generate_triplet_dataset(subjects, relations, objects, batch_size=10000):\n",
    "    \"\"\" 학습 데이터셋을 생성하는 tf.data.Dataset을 구성\n",
    "    \"\"\"\n",
    "    from sklearn.utils import shuffle\n",
    "    subjects, relations, objects = shuffle(subjects, relations, objects)\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    return (tf.data.Dataset\n",
    "            .from_tensor_slices({\"pos_subject\":subjects, \"pos_object\":objects, \"relation\":relations})\n",
    "            .batch(batch_size=batch_size)\n",
    "            .prefetch(AUTOTUNE)        \n",
    "            .map(corrupt_triplet, AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "\n",
    "num_nodes = len(nodes)\n",
    "num_edges = len(edges)\n",
    "embed_size = 50\n",
    "\n",
    "# 초기화 방식은 논문에 나와있는 방식으로 구성\n",
    "init_range = 6/np.sqrt(embed_size)\n",
    "init_op = RandomUniform(-init_range, init_range)\n",
    "\n",
    "node_embed_layer = Embedding(input_dim=num_nodes,\n",
    "                             output_dim=embed_size,\n",
    "                             embeddings_initializer=init_op,\n",
    "                             name='node_embed_layer')\n",
    "edge_embed_layer = Embedding(input_dim=num_edges, \n",
    "                             output_dim=embed_size,\n",
    "                             embeddings_initializer=init_op,\n",
    "                             name='edge_embed_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 층 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 입력층 구성\n",
    "pos_sub_inputs = Input(shape=(), name='pos_subject')\n",
    "neg_sub_inputs = Input(shape=(), name='neg_subject')\n",
    "pos_obj_inputs = Input(shape=(), name='pos_object')\n",
    "neg_obj_inputs = Input(shape=(), name='neg_object')\n",
    "rel_inputs = Input(shape=(), name='relation')\n",
    "\n",
    "# 입력층을 임베딩층으로 연결\n",
    "pos_sub_embed = K.l2_normalize(node_embed_layer(pos_sub_inputs),axis=1)\n",
    "neg_sub_embed = K.l2_normalize(node_embed_layer(neg_sub_inputs),axis=1)\n",
    "pos_obj_embed = K.l2_normalize(node_embed_layer(pos_obj_inputs),axis=1)\n",
    "neg_obj_embed = K.l2_normalize(node_embed_layer(neg_obj_inputs),axis=1)\n",
    "rel_embed = edge_embed_layer(rel_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 손실함수 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(src_embed, dst_embed, norm='l1'):\n",
    "    \"\"\"\n",
    "    src_embed와 dst_embed의 거리\n",
    "    \n",
    "    src_embed : subject + relation\n",
    "    dst_embed : object\n",
    "    \"\"\"\n",
    "    if norm == 'l1':\n",
    "        return K.sum(K.abs(src_embed-dst_embed),1)\n",
    "    elif norm == 'l2':\n",
    "        return K.sum(K.square(src_embed-dst_embed),1)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "\n",
    "pos_dist = distance(pos_sub_embed+rel_embed,pos_obj_embed)\n",
    "neg_dist = distance(neg_sub_embed+rel_embed,neg_obj_embed)\n",
    "\n",
    "margin = 1\n",
    "loss = K.maximum(margin + pos_dist - neg_dist, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "inputs = (pos_sub_inputs, neg_sub_inputs, \n",
    "          pos_obj_inputs, neg_obj_inputs, rel_inputs)\n",
    "\n",
    "model = Model(inputs, loss)\n",
    "model.add_loss(loss)\n",
    "model.compile(optimizer=Adagrad(2e-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 1024\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    triplets = generate_triplet_dataset(\n",
    "        subjects, relations, objects, batch_size)\n",
    "    model.fit(x=triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embed = model.get_layer('node_embed_layer').get_weights()[0]\n",
    "l2_norm = np.linalg.norm(node_embed,ord=2,axis=1)[:,None]\n",
    "node_normalized = node_embed / l2_norm\n",
    "node_df = pd.DataFrame(node_normalized)\n",
    "node_df.index = node_df.index.map(id2node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_df = node_df[node_df.index.isin(train_df.object.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torvalds/linux                                1.000000\n",
       "git/git                                       0.557233\n",
       "GNOME/gimp                                    0.488659\n",
       "aamine/cbc                                    0.473231\n",
       "robotframework/RIDE                           0.471318\n",
       "lupoDharkael/flameshot                        0.465037\n",
       "torproject/tor                                0.464315\n",
       "ikatyang/emoji-cheat-sheet                    0.450000\n",
       "mszep/pandoc_resume                           0.447250\n",
       "Mohist-Community/Mohist                       0.437677\n",
       "cezanne/usbip-win                             0.437552\n",
       "morris821028/UVa                              0.437386\n",
       "pbatard/rufus                                 0.437079\n",
       "nirewen/discord-netflix                       0.428762\n",
       "baldengineer/bit-preserve                     0.426989\n",
       "ellisonleao/magictools                        0.426127\n",
       "PFCraft/Mohist                                0.426030\n",
       "python/cpython                                0.422073\n",
       "DeadManWalkingTO/NVidiaProfileInspectorDmW    0.413647\n",
       "nrandecker/particle                           0.412020\n",
       "dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    repository_df\n",
    "    .dot(repository_df.loc['torvalds/linux'])\n",
    "    .sort_values(ascending=False)\n",
    "    .iloc[:20]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "\n",
    "`torvalds/linux` 같은 리파짓토리와 관련된 활동을 전개한 유저들만을 우선 뽑고, 그들의 활동 내역을 Knowledge Graph 형식으로 구성한 것입니다. Graph Embedding이 유효하게 동작하는지를 해당 Knowledge Graph 데이터로 확인해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_github_kg_dataset(name='linux'):\n",
    "    \"\"\"knowledge graph Dataset을 불러오는 함수\n",
    "    현재 3가지 github knowledge graph가 구성되어 있음\n",
    "    name : linux, tensorflow, vim \n",
    "    \"\"\"\n",
    "    from tensorflow.keras.utils import get_file\n",
    "    fpath = get_file(\"github-playground.h5\",\n",
    "                     \"https://storage.googleapis.com/github-playground/playground.h5\")\n",
    "    target_df = pd.read_hdf(fpath, key=name)    \n",
    "    \n",
    "    type_df = pd.read_hdf(fpath, key='type')\n",
    "    target_df.type = target_df.type.map(type_df.type.to_dict())\n",
    "    \n",
    "    repository_df = pd.read_hdf(fpath, key='repository')\n",
    "    df = pd.merge(target_df, repository_df)\n",
    "    \n",
    "    df.rename({\n",
    "        \"actor_id\": 'subject',\n",
    "        \"type\": 'relation', \n",
    "        \"repo_name\":\"object\"}, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow, vim 도 가능합니다.\n",
    "df = load_github_kg_dataset(name='linux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***caution*** : 아래와 같은 에러가 발생시, 링크를 타고 수정해주세요\n",
    "\n",
    "* [ValueError: cannot set WRITEABLE flag to True of this array](https://github.com/pandas-dev/pandas/issues/24839)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 파이프라인 구축\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요한 Event 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_relations(df, event_types):    \n",
    "    return df[df.relation.isin(event_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding에 활용할 relation type을 지정\n",
    "event_types = ('WatchEvent','PushEvent','IssuesEvent')\n",
    "\n",
    "df = trim_relations(df, event_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Core Sampling 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcore_sampling(df, k_core=5):\n",
    "    for i in range(100):\n",
    "        prev_counts = len(df)\n",
    "        if prev_counts == 0:\n",
    "            raise ValueError(\"No data remains\")\n",
    "        \n",
    "        sub_counts = df.subject.value_counts()\n",
    "        obj_counts = df.object.value_counts()\n",
    "        df = df[df.subject.isin(sub_counts[sub_counts>=k_core].index)\n",
    "                & df.object.isin(obj_counts[obj_counts>=k_core].index)]\n",
    "\n",
    "        if prev_counts == len(df):\n",
    "            # 변화가 없으면 종료\n",
    "            return df\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kcore_sampling(df, k_core=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling 함수\n",
    "\n",
    "> Both datasets contain only positive triplets. As in Bordes et al, we generated negatives using the local closed world assumption. That is, for a triple, we randomly change either the subject or the object at random, to form a negative example. ***This negative sampling is performed at runtime for each batch of training positive examples.***\n",
    "\n",
    "배치 단위로 Negative Sampling하는 것이 핵심\n",
    "\n",
    "- Complex Embeddings for Simple Link Prediction(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.utils import shuffle\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "\n",
    "class GraphDataPipelineBuilder:\n",
    "    logger = logging.get_logger(\"reco_training_service\")\n",
    "    __dir__ = [\"create_complex_dataset\", \"create_transe_dataset\"]\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 graph_dataframe:pd.DataFrame,\n",
    "                 sub_col_name='subject', \n",
    "                 rel_col_name='relation',\n",
    "                 obj_col_name='object'):\n",
    "        self.df = graph_dataframe\n",
    "        assert sub_col_name in self.df.columns\n",
    "        assert rel_col_name in self.df.columns        \n",
    "        assert obj_col_name in self.df.columns\n",
    "\n",
    "        self._initialize_node_index(sub_col_name, obj_col_name)\n",
    "        self._initialize_edge_index(rel_col_name)\n",
    "        self._initialize_dataset(sub_col_name, rel_col_name, obj_col_name)\n",
    "        \n",
    "    def _initialize_node_index(self, subject_col_name, object_col_name):\n",
    "        self.logger.info(\"start to initialize node index\")\n",
    "        nodes = (set(self.df[subject_col_name].unique()) \n",
    "                 | set(self.df[object_col_name].unique()))\n",
    "\n",
    "        self.node2id = {node:i for i, node in enumerate(nodes)}\n",
    "        self.num_nodes = len(nodes)\n",
    "        \n",
    "        id2node = {i:node for i, node in enumerate(nodes)}\n",
    "        self.node_index = pd.Series(id2node).to_frame()\n",
    "        \n",
    "        \n",
    "    def _initialize_edge_index(self, relation_col_name):\n",
    "        self.logger.info(\"start to initialize edge index\")\n",
    "        edges = set(self.df[relation_col_name].unique())\n",
    "\n",
    "        self.edge2id = {edge:i for i, edge in enumerate(edges)}\n",
    "        self.num_edges = len(edges)\n",
    "        \n",
    "        id2edge = {i:edge for i, edge in enumerate(edges)}\n",
    "        self.edge_index = pd.Series(id2edge).to_frame()\n",
    "        \n",
    "    def _initialize_dataset(self, \n",
    "                            sub_col_name, \n",
    "                            rel_col_name, \n",
    "                            obj_col_name):\n",
    "        self.logger.info(\"start to initialize dataset\")\n",
    "        subs = self.df[sub_col_name].map(self.node2id).values\n",
    "        rels = self.df[rel_col_name].map(self.edge2id).values\n",
    "        objs = self.df[obj_col_name].map(self.node2id).values\n",
    "\n",
    "        subs, rels, objs = shuffle(subs, rels, objs)\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices({\n",
    "                \"subject\":subs, \"object\":objs, \"relation\":rels})\n",
    "        \n",
    "    def create_complex_dataset(self, batch_size, num_neg):\n",
    "        return (self.dataset\n",
    "                .shuffle(batch_size*1000)\n",
    "                .batch(batch_size)\n",
    "                .map(complEx_negative_sampler(num_neg), AUTOTUNE))\n",
    "    \n",
    "    def create_transe_dataset(self, batch_size):\n",
    "        return (self.dataset\n",
    "                .shuffle(batch_size*1000)\n",
    "                .batch(batch_size)\n",
    "                .map(transE_negative_sampler, AUTOTUNE))\n",
    "    \n",
    "\n",
    "def complEx_negative_sampler(num_negs):\n",
    "    \"\"\"Edge Negative Sampling strategy in complEx Model\n",
    "    params : \n",
    "        * num_neg: 1 positive sample 당 negative 비율\n",
    "    \"\"\"\n",
    "    def sampler(triplet):\n",
    "        t = triplet\n",
    "        p_rel, n_rel = t['relation'], tf.tile(t['relation'], [num_negs])\n",
    "        p_sub, n_sub = t['subject'], tf.tile(t['subject'], [num_negs])\n",
    "        p_obj, n_obj = t['object'], tf.tile(t['object'], [num_negs])\n",
    "\n",
    "        n_sub, n_obj = corrupt_head_or_tail(n_sub, n_obj)\n",
    "\n",
    "        inputs = {'relation': tf.concat([p_rel, n_rel], axis=-1),\n",
    "                  'subject' : tf.concat([p_sub, n_sub], axis=-1),\n",
    "                  'object'  : tf.concat([p_obj, n_obj], axis=-1)}\n",
    "\n",
    "        p_labels, n_labels = tf.ones_like(p_rel), tf.zeros_like(n_rel)\n",
    "        labels = tf.concat([p_labels, n_labels], axis=-1)                                              \n",
    "\n",
    "        return inputs, labels\n",
    "    return sampler\n",
    "        \n",
    "def transE_negative_sampler(triplets):\n",
    "    \"\"\"Edge Negative Sampling strategy in transE Model\n",
    "    \"\"\"\n",
    "    t = triplet\n",
    "    p_sub, p_obj, rel = t['subject'], t['object'], t['relation']\n",
    "    n_sub, n_obj = corrupt_head_or_tail(p_sub, p_obj) \n",
    "    return {\"pos_subject\": p_sub, \"neg_subject\": n_sub,\n",
    "            \"pos_object\": p_obj, \"neg_object\": n_obj, \n",
    "            \"relation\": relation}\n",
    "    \n",
    "def corrupt_head_or_tail(heads, tails):\n",
    "    \"\"\" 50% 확률로 head 혹은 tail을 corrupt\n",
    "    \"\"\"        \n",
    "    h_flag = tf.random.uniform(tf.shape(heads)) < 0.5\n",
    "\n",
    "    neg_heads = tf.where(\n",
    "        h_flag, heads, tf.random.shuffle(heads))\n",
    "    neg_tails = tf.where(\n",
    "        h_flag, tf.random.shuffle(tails), tails)    \n",
    "    return neg_heads, neg_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to initialize node index\n",
      "Start to initialize edge index\n",
      "Start to initialize dataset\n"
     ]
    }
   ],
   "source": [
    "builder = GraphDataPipelineBuilder(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Score Function`\n",
    "\n",
    "$\n",
    "\\phi(r,s,o;\\Theta) = Re(<w_r, e_s, \\bar e_{o}> \\\\\n",
    "= Re(\\sum ^{K}_{k=1} W_{rk} e_{sk} \\bar e_{ok}) \\\\\n",
    "= <Re(w_r), Re(e_s), Re(e_o)> \\\\\n",
    "  + <Re(w_r), Im(e_s), Im(e_o)> \\\\\n",
    "  + <Im(w_r), Re(e_s), Im(e_o)> \\\\\n",
    "  - <Im(w_r), Im(e_s), Re(e_o)> \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class ComplexDotScore(Layer):\n",
    "    \"\"\" complEx Scoring Function\n",
    "        - Based on Hermitian (or sesquilinear) dot product\n",
    "        - score = Re(<relation, subject, object>)\n",
    "        - Embedding의 구성\n",
    "           * embed[:,:len(embed)//2] : real-value\n",
    "           * embed[:,len(embed)//2:] : imaginary-value\n",
    "    \"\"\"\n",
    "    def __init__(self, l2_reg=0., **kwargs):\n",
    "        self.l2_reg = l2_reg\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        rel, sub, obj = inputs\n",
    "        \n",
    "        if self.l2_reg:\n",
    "            l2_loss = K.mean(K.sum(rel**2+sub**2+obj**2, axis=1))\n",
    "            self.add_loss(self.l2_reg * l2_loss)\n",
    "        \n",
    "        re_rel, im_rel = tf.split(rel, 2, axis=-1)\n",
    "        re_sub, im_sub = tf.split(sub, 2, axis=-1)\n",
    "        re_obj, im_obj = tf.split(obj, 2, axis=-1)\n",
    "        return K.sum(  re_rel * re_sub * re_obj\n",
    "                     + re_rel * im_sub * im_obj\n",
    "                     + im_rel * re_sub * im_obj\n",
    "                     - im_rel * im_sub * re_obj, axis=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_complex_model(num_nodes, num_edges, embed_size=50):\n",
    "    sub_inputs = Input(shape=(), name='subject')\n",
    "    obj_inputs = Input(shape=(), name='object')\n",
    "    rel_inputs = Input(shape=(), name='relation')\n",
    "\n",
    "    node_embed_layer = Embedding(input_dim=num_nodes,\n",
    "                                 output_dim=embed_size,\n",
    "                                 embeddings_initializer=GlorotUniform(),\n",
    "                                 name='node_embed_layer')\n",
    "    edge_embed_layer = Embedding(input_dim=num_edges, \n",
    "                                 output_dim=embed_size,\n",
    "                                 embeddings_initializer=GlorotUniform(),\n",
    "                                 name='edge_embed_layer')\n",
    "\n",
    "    sub_embed = node_embed_layer(sub_inputs)\n",
    "    obj_embed = node_embed_layer(obj_inputs)\n",
    "    rel_embed = edge_embed_layer(rel_inputs)\n",
    "\n",
    "    outputs = ComplexDotScore(l2_reg=1e-3)([rel_embed, sub_embed, obj_embed])\n",
    "    \n",
    "    model = Model({\n",
    "        \"subject\":sub_inputs, \"object\":obj_inputs, \"relation\":rel_inputs}, \n",
    "        outputs, name='complEx')\n",
    "    return model\n",
    "    \n",
    "model = build_complex_model(num_nodes=builder.num_nodes,\n",
    "                            num_edges=builder.num_edges,\n",
    "                            embed_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성\n",
    "\n",
    "> Models were trained using Stochastic Gradient Descent with mini-batches and AdaGrad for tuning the learning rate, by minimizing the negative log-likelihood of the logistic model with l2 regularization on the parameters theta of the considered model.\n",
    "\n",
    "\n",
    "하지만 많은 구현체에서는 BinaryCrossEntropy 대신 Softplus를 보다 선호하는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "lr = 5e-1\n",
    "\n",
    "loss = BinaryCrossentropy(from_logits=True,\n",
    "                          reduction='sum')\n",
    "model.compile(optimizer=Adagrad(lr), loss=loss, metrics=[loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Adagrad/iter:0' shape=() dtype=int64, numpy=321>,\n",
       " <tf.Variable 'Adagrad/edge_embed_layer/embeddings/accumulator:0' shape=(3, 50) dtype=float32, numpy=\n",
       " array([[1.85931416e+01, 1.11920137e+01, 1.06733980e+01, 7.50183296e+00,\n",
       "         8.75904751e+00, 1.52616177e+01, 2.12846012e+01, 2.52804527e+01,\n",
       "         4.96522856e+00, 7.07790613e+00, 7.31607246e+00, 1.97377930e+01,\n",
       "         7.61124992e+00, 8.38400841e+00, 3.17214108e+01, 1.62495289e+01,\n",
       "         6.06527233e+00, 2.01831436e+01, 4.89428558e+01, 5.56989193e+00,\n",
       "         9.26418781e+00, 1.43128757e+01, 1.33969231e+01, 6.10937071e+00,\n",
       "         1.78381767e+01, 1.61454735e+01, 1.44989624e+01, 9.00804615e+00,\n",
       "         7.40958881e+00, 1.46221952e+01, 1.47404375e+01, 3.86845474e+01,\n",
       "         1.32934265e+01, 5.27309370e+00, 5.79716587e+00, 9.24130821e+00,\n",
       "         1.81025467e+01, 5.28130579e+00, 6.69307089e+00, 1.58964024e+01,\n",
       "         1.46895475e+01, 6.33017445e+00, 2.68564606e+01, 2.30666943e+01,\n",
       "         6.08544207e+00, 7.03345633e+00, 2.78595047e+01, 1.66207294e+01,\n",
       "         5.19013691e+00, 1.79511070e+01],\n",
       "        [2.60317383e+02, 1.60174942e+02, 1.42174515e+02, 1.41060150e+02,\n",
       "         1.32173325e+02, 2.66266693e+02, 5.47089783e+02, 4.00504395e+02,\n",
       "         1.08008614e+02, 1.26534721e+02, 2.11943085e+02, 2.67482452e+02,\n",
       "         1.48603897e+02, 1.64962631e+02, 6.84043640e+02, 2.46772659e+02,\n",
       "         1.19681664e+02, 4.39336243e+02, 6.73629150e+02, 1.42466873e+02,\n",
       "         1.36119919e+02, 2.30974930e+02, 2.55512253e+02, 1.15997490e+02,\n",
       "         1.92953232e+02, 2.04104172e+02, 2.98857635e+02, 1.65505081e+02,\n",
       "         1.79237305e+02, 2.32156433e+02, 2.25238739e+02, 8.47953674e+02,\n",
       "         1.82056046e+02, 1.02673805e+02, 1.13880066e+02, 1.84282837e+02,\n",
       "         3.75909180e+02, 1.13930733e+02, 1.15594452e+02, 3.39113007e+02,\n",
       "         3.55465698e+02, 1.63764114e+02, 4.82359558e+02, 6.51976685e+02,\n",
       "         1.47269974e+02, 1.15982544e+02, 5.56111389e+02, 2.98735718e+02,\n",
       "         1.25422417e+02, 2.35922333e+02],\n",
       "        [4.20369570e+04, 3.67994648e+04, 3.20158418e+04, 2.77040938e+04,\n",
       "         2.67808418e+04, 6.60471797e+04, 4.22354312e+05, 1.92453547e+05,\n",
       "         2.73652129e+04, 3.21752500e+04, 1.83741836e+04, 1.31970422e+05,\n",
       "         2.51176816e+04, 3.10092871e+04, 3.28047719e+05, 7.98663516e+04,\n",
       "         2.79004316e+04, 7.17622109e+04, 5.20935406e+05, 2.52099961e+04,\n",
       "         2.74829180e+04, 5.33161953e+04, 1.01327961e+05, 2.72142676e+04,\n",
       "         3.88158711e+04, 4.44659180e+04, 1.12737930e+05, 3.29035078e+04,\n",
       "         2.46794922e+04, 6.86405312e+04, 7.73868672e+04, 7.41935062e+05,\n",
       "         3.31207305e+04, 1.81565781e+04, 2.62350410e+04, 1.69468516e+04,\n",
       "         8.47273281e+04, 2.83713066e+04, 2.51888633e+04, 1.69849984e+05,\n",
       "         1.16067914e+05, 2.41717773e+04, 4.97063438e+04, 1.07142138e+06,\n",
       "         3.01114863e+04, 2.23050840e+04, 1.59833844e+05, 1.50623812e+05,\n",
       "         2.42886309e+04, 6.75818906e+04]], dtype=float32)>,\n",
       " <tf.Variable 'Adagrad/node_embed_layer/embeddings/accumulator:0' shape=(463656, 50) dtype=float32, numpy=\n",
       " array([[0.1       , 0.1       , 0.1       , ..., 0.1       , 0.1       ,\n",
       "         0.1       ],\n",
       "        [0.1       , 0.1       , 0.1       , ..., 0.1       , 0.1       ,\n",
       "         0.1       ],\n",
       "        [0.10112146, 0.12269965, 0.10036363, ..., 0.10249721, 0.10031523,\n",
       "         0.10479286],\n",
       "        ...,\n",
       "        [0.1       , 0.1       , 0.1       , ..., 0.1       , 0.1       ,\n",
       "         0.1       ],\n",
       "        [0.10087859, 0.1002382 , 0.10060709, ..., 0.10248209, 0.10007946,\n",
       "         0.10231852],\n",
       "        [0.10502212, 0.10463077, 0.10621866, ..., 0.11174826, 0.10100563,\n",
       "         0.10831625]], dtype=float32)>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 320/2633 [==>...........................] - ETA: 8:23 - loss: 986.6066 - binary_crossentropy: 0.1204"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-7fc60c6aabf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m model.fit(trainset,\n\u001b[1;32m     13\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           class_weight={1:1., 0:1/num_negs})\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "num_epochs = 100\n",
    "num_negs = 20\n",
    "batch_size = 4096\n",
    "\n",
    "trainset =(builder.dataset\n",
    "           .shuffle(batch_size*1000)       \n",
    "           .batch(batch_size)\n",
    "           .map(complEx_negative_sampler(num_negs), AUTOTUNE))\n",
    "\n",
    "model.fit(trainset,\n",
    "          epochs=num_epochs,\n",
    "          class_weight={1:1., 0:1/num_negs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embed = model.get_layer('node_embed_layer').get_weights()[0]\n",
    "l2_norm = np.linalg.norm(node_embed,ord=2,axis=1)[:,None]\n",
    "node_normalized = node_embed / l2_norm\n",
    "node_df = pd.DataFrame(node_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.index = builder.node_index.values[:,0].astype(str)\n",
    "repository_df = node_df[node_df.index.str.contains('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "benoitc/gunicorn                            66.528847\n",
       "niwinz/django-redis                         54.609261\n",
       "django/channels                             53.396442\n",
       "marcgibbons/django-rest-swagger             52.900311\n",
       "ottoyiu/django-cors-headers                 52.062054\n",
       "django/daphne                               51.935993\n",
       "unbit/uwsgi                                 50.635353\n",
       "gevent/gevent                               49.102066\n",
       "celery/py-amqp                              48.963509\n",
       "dabeaz/curio                                48.034958\n",
       "jpadilla/django-rest-framework-jwt          47.915226\n",
       "python/pythondotorg                         47.538326\n",
       "aiortc/aioquic                              47.478580\n",
       "Tivix/django-cron                           47.278633\n",
       "celery/kombu                                47.041466\n",
       "MagicStack/uvloop                           46.975761\n",
       "miguelgrinberg/Flask-SocketIO               46.371864\n",
       "adamchainz/django-mysql                     46.239624\n",
       "davesque/django-rest-framework-simplejwt    45.596004\n",
       "Tivix/django-rest-auth                      45.254448\n",
       "dtype: float32"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    repository_df\n",
    "    .dot(repository_df.loc['benoitc/gunicorn'])\n",
    "    .sort_values(ascending=False)\n",
    "    .iloc[:20]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 손실함수 변경\n",
    "* https://arxiv.org/pdf/1806.07297.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

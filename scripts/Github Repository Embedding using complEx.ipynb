{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "\n",
    "`torvalds/linux` 같은 리파짓토리와 관련된 활동을 전개한 유저들만을 우선 뽑고, 그들의 활동 내역을 Knowledge Graph 형식으로 구성한 것입니다. Graph Embedding이 유효하게 동작하는지를 해당 Knowledge Graph 데이터로 확인해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_github_kg_dataset(name='linux'):\n",
    "    \"\"\"knowledge graph Dataset을 불러오는 함수\n",
    "    현재 3가지 github knowledge graph가 구성되어 있음\n",
    "    name : linux, tensorflow, vim \n",
    "    \"\"\"\n",
    "    from tensorflow.keras.utils import get_file\n",
    "    fpath = get_file(\"github-playground.h5\",\n",
    "                     \"https://storage.googleapis.com/github-playground/playground.h5\")\n",
    "    target_df = pd.read_hdf(fpath, key=name)    \n",
    "    \n",
    "    type_df = pd.read_hdf(fpath, key='type')\n",
    "    target_df.type = target_df.type.map(type_df.type.to_dict())\n",
    "    \n",
    "    repository_df = pd.read_hdf(fpath, key='repository')\n",
    "    df = pd.merge(target_df, repository_df)\n",
    "    \n",
    "    df.rename({\n",
    "        \"actor_id\": 'subject',\n",
    "        \"type\": 'relation', \n",
    "        \"repo_name\":\"object\"}, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow, vim 도 가능합니다.\n",
    "df = load_github_kg_dataset(name='linux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***caution*** : 아래와 같은 에러가 발생시, 링크를 타고 수정해주세요\n",
    "\n",
    "* [ValueError: cannot set WRITEABLE flag to True of this array](https://github.com/pandas-dev/pandas/issues/24839)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 파이프라인 구축\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불필요한 Event 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_relations(df, event_types):    \n",
    "    return df[df.relation.isin(event_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding에 활용할 relation type을 지정\n",
    "event_types = ('WatchEvent','PushEvent','IssuesEvent')\n",
    "\n",
    "df = trim_relations(df, event_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Core Sampling 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcore_sampling(df, k_core=5):\n",
    "    for i in range(100):\n",
    "        prev_counts = len(df)\n",
    "        if prev_counts == 0:\n",
    "            raise ValueError(\"No data remains\")\n",
    "        \n",
    "        sub_counts = df.subject.value_counts()\n",
    "        obj_counts = df.object.value_counts()\n",
    "        df = df[df.subject.isin(sub_counts[sub_counts>=k_core].index)\n",
    "                & df.object.isin(obj_counts[obj_counts>=k_core].index)]\n",
    "\n",
    "        if prev_counts == len(df):\n",
    "            # 변화가 없으면 종료\n",
    "            return df\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kcore_sampling(df, k_core=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling 함수\n",
    "\n",
    "> Both datasets contain only positive triplets. As in Bordes et al, we generated negatives using the local closed world assumption. That is, for a triple, we randomly change either the subject or the object at random, to form a negative example. ***This negative sampling is performed at runtime for each batch of training positive examples.***\n",
    "\n",
    "배치 단위로 Negative Sampling하는 것이 핵심\n",
    "\n",
    "- Complex Embeddings for Simple Link Prediction(paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.utils import shuffle\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "class GraphDataPipelineBuilder:\n",
    "    logger = logging.getLogger(\"reco_training_service\")\n",
    "    __dir__ = [\"create_complex_dataset\", \"create_transe_dataset\"]\n",
    "    \n",
    "    def __init__(self, \n",
    "                 graph_dataframe:pd.DataFrame,\n",
    "                 sub_col_name='subject', \n",
    "                 rel_col_name='relation',\n",
    "                 obj_col_name='object'):\n",
    "        self.df = graph_dataframe\n",
    "        assert sub_col_name in self.df.columns\n",
    "        assert rel_col_name in self.df.columns        \n",
    "        assert obj_col_name in self.df.columns\n",
    "\n",
    "        self._initialize_node_index(sub_col_name, obj_col_name)\n",
    "        self._initialize_edge_index(rel_col_name)\n",
    "        self._initialize_dataset(sub_col_name, rel_col_name, obj_col_name)\n",
    "        \n",
    "    def _initialize_node_index(self, subject_col_name, object_col_name):\n",
    "        self.logger.info(\"start to initialize node index\")\n",
    "        nodes = (set(self.df[subject_col_name].unique()) \n",
    "                 | set(self.df[object_col_name].unique()))\n",
    "\n",
    "        self.node2id = {node:i for i, node in enumerate(nodes)}\n",
    "        self.num_nodes = len(nodes)\n",
    "        \n",
    "        id2node = {i:node for i, node in enumerate(nodes)}\n",
    "        self.node_index = pd.Series(id2node).to_frame()\n",
    "        \n",
    "        \n",
    "    def _initialize_edge_index(self, relation_col_name):\n",
    "        self.logger.info(\"start to initialize edge index\")\n",
    "        edges = set(self.df[relation_col_name].unique())\n",
    "\n",
    "        self.edge2id = {edge:i for i, edge in enumerate(edges)}\n",
    "        self.num_edges = len(edges)\n",
    "        \n",
    "        id2edge = {i:edge for i, edge in enumerate(edges)}\n",
    "        self.edge_index = pd.Series(id2edge).to_frame()\n",
    "        \n",
    "    def _initialize_dataset(self, \n",
    "                            sub_col_name, \n",
    "                            rel_col_name, \n",
    "                            obj_col_name):\n",
    "        self.logger.info(\"start to initialize dataset\")\n",
    "        subs = self.df[sub_col_name].map(self.node2id).values\n",
    "        rels = self.df[rel_col_name].map(self.edge2id).values\n",
    "        objs = self.df[obj_col_name].map(self.node2id).values\n",
    "\n",
    "        subs, rels, objs = shuffle(subs, rels, objs)\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices({\n",
    "                \"subject\":subs, \"object\":objs, \"relation\":rels})\n",
    "        \n",
    "    def create_complex_dataset(self, batch_size, num_neg):\n",
    "        return (self.dataset\n",
    "                .shuffle(batch_size*1000)\n",
    "                .batch(batch_size)\n",
    "                .map(complEx_negative_sampler(num_neg), AUTOTUNE))\n",
    "    \n",
    "    def create_transe_dataset(self, batch_size):\n",
    "        return (self.dataset\n",
    "                .shuffle(batch_size*1000)\n",
    "                .batch(batch_size)\n",
    "                .map(transE_negative_sampler, AUTOTUNE))\n",
    "    \n",
    "\n",
    "def complEx_negative_sampler(num_negs):\n",
    "    \"\"\"Edge Negative Sampling strategy in complEx Model\n",
    "    params : \n",
    "        * num_neg: 1 positive sample 당 negative 비율\n",
    "    \"\"\"\n",
    "    def sampler(triplet):\n",
    "        t = triplet\n",
    "        p_rel, n_rel = t['relation'], tf.tile(t['relation'], [num_negs])\n",
    "        p_sub, n_sub = t['subject'], tf.tile(t['subject'], [num_negs])\n",
    "        p_obj, n_obj = t['object'], tf.tile(t['object'], [num_negs])\n",
    "\n",
    "        n_sub, n_obj = corrupt_head_or_tail(n_sub, n_obj)\n",
    "\n",
    "        inputs = {'relation': tf.concat([p_rel, n_rel], axis=-1),\n",
    "                  'subject' : tf.concat([p_sub, n_sub], axis=-1),\n",
    "                  'object'  : tf.concat([p_obj, n_obj], axis=-1)}\n",
    "\n",
    "        p_labels, n_labels = tf.ones_like(p_rel), tf.zeros_like(n_rel)\n",
    "        labels = tf.concat([p_labels, n_labels], axis=-1)                                              \n",
    "\n",
    "        return inputs, labels\n",
    "    return sampler\n",
    "        \n",
    "def transE_negative_sampler(triplets):\n",
    "    \"\"\"Edge Negative Sampling strategy in transE Model\n",
    "    \"\"\"\n",
    "    t = triplet\n",
    "    p_sub, p_obj, rel = t['subject'], t['object'], t['relation']\n",
    "    n_sub, n_obj = corrupt_head_or_tail(p_sub, p_obj) \n",
    "    return {\"pos_subject\": p_sub, \"neg_subject\": n_sub,\n",
    "            \"pos_object\": p_obj, \"neg_object\": n_obj, \n",
    "            \"relation\": relation}\n",
    "    \n",
    "def corrupt_head_or_tail(heads, tails):\n",
    "    \"\"\" 50% 확률로 head 혹은 tail을 corrupt\n",
    "    \"\"\"        \n",
    "    h_flag = tf.random.uniform(tf.shape(heads)) < 0.5\n",
    "\n",
    "    neg_heads = tf.where(\n",
    "        h_flag, heads, tf.random.shuffle(heads))\n",
    "    neg_tails = tf.where(\n",
    "        h_flag, tf.random.shuffle(tails), tails)    \n",
    "    return neg_heads, neg_tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = GraphDataPipelineBuilder(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Score Function`\n",
    "\n",
    "$\n",
    "\\phi(r,s,o;\\Theta) = Re(<w_r, e_s, \\bar e_{o}> \\\\\n",
    "= Re(\\sum ^{K}_{k=1} W_{rk} e_{sk} \\bar e_{ok}) \\\\\n",
    "= <Re(w_r), Re(e_s), Re(e_o)> \\\\\n",
    "  + <Re(w_r), Im(e_s), Im(e_o)> \\\\\n",
    "  + <Im(w_r), Re(e_s), Im(e_o)> \\\\\n",
    "  - <Im(w_r), Im(e_s), Re(e_o)> \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class ComplexDotScore(Layer):\n",
    "    \"\"\" complEx Scoring Function\n",
    "        - Based on Hermitian (or sesquilinear) dot product\n",
    "        - score = Re(<relation, subject, object>)\n",
    "        - Embedding의 구성\n",
    "           * embed[:,:len(embed)//2] : real-value\n",
    "           * embed[:,len(embed)//2:] : imaginary-value\n",
    "    \"\"\"\n",
    "    def __init__(self, l3_reg=0., **kwargs):\n",
    "        self.l3_reg = l3_reg\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        rel, sub, obj = inputs\n",
    "        \n",
    "        if self.l3_reg:\n",
    "            l3_loss = K.mean(K.sum(\n",
    "                K.abs(rel)**3+K.abs(sub)**3+K.abs(obj)**3, axis=1))\n",
    "            self.add_loss(self.l3_reg * l3_loss)\n",
    "        \n",
    "        re_rel, im_rel = tf.split(rel, 2, axis=-1)\n",
    "        re_sub, im_sub = tf.split(sub, 2, axis=-1)\n",
    "        re_obj, im_obj = tf.split(obj, 2, axis=-1)\n",
    "        return K.sum(  re_rel * re_sub * re_obj\n",
    "                     + re_rel * im_sub * im_obj\n",
    "                     + im_rel * re_sub * im_obj\n",
    "                     - im_rel * im_sub * re_obj, axis=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_complex_model(num_nodes, num_edges, embed_size=50):\n",
    "    sub_inputs = Input(shape=(), name='subject')\n",
    "    obj_inputs = Input(shape=(), name='object')\n",
    "    rel_inputs = Input(shape=(), name='relation')\n",
    "\n",
    "    node_embed_layer = Embedding(input_dim=num_nodes,\n",
    "                                 output_dim=embed_size,\n",
    "                                 embeddings_initializer=GlorotUniform(),\n",
    "                                 name='node_embed_layer')\n",
    "    edge_embed_layer = Embedding(input_dim=num_edges, \n",
    "                                 output_dim=embed_size,\n",
    "                                 embeddings_initializer=GlorotUniform(),\n",
    "                                 name='edge_embed_layer')\n",
    "\n",
    "    sub_embed = node_embed_layer(sub_inputs)\n",
    "    obj_embed = node_embed_layer(obj_inputs)\n",
    "    rel_embed = edge_embed_layer(rel_inputs)\n",
    "\n",
    "    outputs = ComplexDotScore(l3_reg=1e-3)([rel_embed, sub_embed, obj_embed])\n",
    "    \n",
    "    model = Model({\n",
    "        \"subject\":sub_inputs, \"object\":obj_inputs, \"relation\":rel_inputs}, \n",
    "        outputs, name='complEx')\n",
    "    return model\n",
    "    \n",
    "model = build_complex_model(num_nodes=builder.num_nodes,\n",
    "                            num_edges=builder.num_edges,\n",
    "                            embed_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성\n",
    "\n",
    "> Models were trained using Stochastic Gradient Descent with mini-batches and AdaGrad for tuning the learning rate, by minimizing the negative log-likelihood of the logistic model with l2 regularization on the parameters theta of the considered model.\n",
    "\n",
    "\n",
    "하지만 많은 구현체에서는 BinaryCrossEntropy 대신 Softplus를 보다 선호하는 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "lr = 5e-1\n",
    "\n",
    "loss = BinaryCrossentropy(from_logits=True,\n",
    "                          reduction='sum')\n",
    "model.compile(optimizer=Adagrad(lr), loss=loss, metrics=[loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "num_epochs = 100\n",
    "num_negs = 20\n",
    "batch_size = 4096\n",
    "\n",
    "model.fit(builder.create_complex_dataset(batch_size, num_negs),\n",
    "          epochs=num_epochs, class_weight={1:1., 0:1/num_negs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embed = model.get_layer('node_embed_layer').get_weights()[0]\n",
    "l2_norm = np.linalg.norm(node_embed,ord=2,axis=1)[:,None]\n",
    "node_normalized = node_embed / l2_norm\n",
    "node_df = pd.DataFrame(node_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.index = builder.node_index.values[:,0].astype(str)\n",
    "repository_df = node_df[node_df.index.str.contains('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    repository_df\n",
    "    .dot(repository_df.loc['torvalds/linux'])\n",
    "    .sort_values(ascending=False)\n",
    "    .iloc[:20]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
